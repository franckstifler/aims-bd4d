{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: ",
     "output_type": "error",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter:",
      "at A.startServer (/home/franckstifler/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:786120)",
      "at async A.ensureServerAndNotebookImpl (/home/franckstifler/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:785575)",
      "at async A.ensureServerAndNotebook (/home/franckstifler/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:785376)",
      "at async A.submitCode (/home/franckstifler/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:782328)",
      "at async A.reexecuteCell (/home/franckstifler/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:75:879318)"
     ]
    }
   ],
   "source": [
    "# A proposition of implementation of a web crawler. Tought and implemented by Franck & Noela BD4D 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import pandas as pd\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TYPES = [\"application/pdf\", \"pdf\", \"word\", \"excel\", \"officedocument\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_visited = {} # This keeps the visited links. A dict is faster to look for key existance than a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = {\"file_name\": [], \"time_taken\": [], \"country\": []} # dict to be turned into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_cd(cd, url):\n",
    "    \"\"\"\n",
    "    Get filename from content-disposition\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        # If content disposition not defined, the name of the file is the last part of the url\n",
    "        if url.find('/'):\n",
    "            return url.rsplit('/', 1)[1]\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_downloadable(response):\n",
    "    \"\"\"\n",
    "    Does the url contain a downloadable resource\n",
    "    \"\"\"\n",
    "    header = response.headers\n",
    "    content_type = header.get('content-type')\n",
    "    return content_type.lower() in CONTENT_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_to_disk(country_code, file_name, content):\n",
    "    file = path = \"{}/{}/{}\".format(os.getcwd(), country_code, file_name)\n",
    "    open(file, 'wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(html, base_url, url):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    parsed_url = urlparse(base_url)\n",
    "    scheme = parsed_url.scheme\n",
    "    root_url = parsed_url.netloc\n",
    "    to_visit = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        new_link = None\n",
    "        if href:\n",
    "            if href[0] == \"/\" :\n",
    "                # example of href = \"/environment.html\n",
    "                new_link = scheme + '://' + root_url + href\n",
    "            elif (root_url in href):\n",
    "                # example of href = \"**root_url**/href\"\n",
    "                new_link = href\n",
    "            elif (not re.match('^https?', href)) and re.match('[a-zA-Z]', href):\n",
    "                # example of href \"home.html\" should exclude link starting with http which may link outside (youtube, facebook...)\n",
    "                next_link = url + \"/\" + href\n",
    "                if root_url in next_link:\n",
    "                    new_link = next_link\n",
    "            if new_link and links_visited.get(new_link) == None:\n",
    "                # this is to avoid visiting the same links repeatedly\n",
    "                to_visit.append(new_link)\n",
    "\n",
    "    return to_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_link(url=None, country_code=\"\", base_url=\"\"):\n",
    "    if links_visited.get(url) != None:\n",
    "        return\n",
    "    else:\n",
    "        print('Visiting: ' + url)\n",
    "        start = time.clock()\n",
    "        links_visited[url] = 1\n",
    "        try:\n",
    "            response = requests.get(url, allow_redirects=True, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                if is_downloadable(response):\n",
    "                    file_name = response.headers.get('content-disposition')\n",
    "                    file_name = get_filename_from_cd(file_name, url)\n",
    "                    print(file_name + ' YEAH')\n",
    "                    write_file_to_disk(country_code, file_name, response.content)\n",
    "                    downloaded_files['country'].append(country_code)\n",
    "                    downloaded_files['file_name'].append(file_name)\n",
    "                    downloaded_files['time_taken'].append((time.clock() - start) * 1000)\n",
    "                    df = pd.DataFrame(downloaded_files)\n",
    "                    # We write to the file each time in other to have results in case of early exit...\n",
    "                    df.to_csv('result.csv', encoding='utf-8', index=False)\n",
    "                else:\n",
    "                    html = response.text\n",
    "                    links_to_visit = parse_links(html, base_url, url)\n",
    "                    for link in links_to_visit:\n",
    "                        visit_link(link, country_code, base_url)\n",
    "                    return 0\n",
    "        except:\n",
    "            print(\"Link not valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_folder(folder_name):\n",
    "    current_path = os.getcwd()\n",
    "    path = \"{}/{}\".format(current_path, folder_name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threads = []\n",
    "def process_countries():\n",
    "    df = pd.read_csv('../data/webLinks.csv')\n",
    "    for index, row in df.iterrows():\n",
    "        # create the folder for the country\n",
    "        create_country_folder(row['countryCode'])\n",
    "        if row['linkAvailable'] == 'Yes':\n",
    "            # I make use of Threads for parallel execution (instead of one country as a loop will normally do)\n",
    "            process = Thread(target=visit_link, args=[row['link'], row['countryCode'], row['link']])\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# We now pause execution on the main thread by 'joining' all of our started threads.\n",
    "# This ensures that each has finished processing the urls.\n",
    "for process in threads:\n",
    "    process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_countries()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}